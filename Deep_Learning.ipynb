{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOX0ha9yAAispEX8lUOmTMv",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/suryateja20031104/dbmsproj/blob/main/Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YICHYeVK883h",
        "outputId": "fb9cffb1-8796-404f-c1c2-8db3021d032f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Deep Learning\n"
          ]
        }
      ],
      "source": [
        "print(\"Deep Learning\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.** Implement a 2 node neural network model using keras**"
      ],
      "metadata": {
        "id": "1tQH90gWDvFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "from tensorflow import keras\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense"
      ],
      "metadata": {
        "id": "EuV8D65T92kG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])"
      ],
      "metadata": {
        "id": "qFlKiIXH-8OL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(Dense(2, input_dim=2, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))"
      ],
      "metadata": {
        "id": "s93xqyg0_JCI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.fit(X, y, epochs=10, batch_size=4)\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\")\n",
        "print(predictions)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-h8TsTM_h6n",
        "outputId": "8a7a8a2c-7adf-4e12-adf8-f0f9e7a60d04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "1/1 [==============================] - 2s 2s/step - loss: 0.4935 - accuracy: 0.7500\n",
            "Epoch 2/10\n",
            "1/1 [==============================] - 0s 12ms/step - loss: 0.4940 - accuracy: 0.7500\n",
            "Epoch 3/10\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4935 - accuracy: 0.7500\n",
            "Epoch 4/10\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4934 - accuracy: 0.7500\n",
            "Epoch 5/10\n",
            "1/1 [==============================] - 0s 9ms/step - loss: 0.4934 - accuracy: 0.7500\n",
            "Epoch 6/10\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4934 - accuracy: 0.7500\n",
            "Epoch 7/10\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4934 - accuracy: 0.7500\n",
            "Epoch 8/10\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4934 - accuracy: 0.7500\n",
            "Epoch 9/10\n",
            "1/1 [==============================] - 0s 15ms/step - loss: 0.4934 - accuracy: 0.7500\n",
            "Epoch 10/10\n",
            "1/1 [==============================] - 0s 10ms/step - loss: 0.4934 - accuracy: 0.7500\n",
            "1/1 [==============================] - 0s 181ms/step - loss: 0.4933 - accuracy: 0.7500\n",
            "1/1 [==============================] - 0s 81ms/step\n",
            "Predictions:\n",
            "[[0.35336965]\n",
            " [0.94064164]\n",
            " [0.35336965]\n",
            " [0.35336965]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. **Implement XOR Problem using Multi-Layered Perceptron**"
      ],
      "metadata": {
        "id": "Jq9EaawBG-7j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense\n",
        "\n",
        "X = np.array([[0, 0], [0, 1], [1, 0], [1, 1]])\n",
        "y = np.array([[0], [1], [1], [0]])\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Dense(8, input_dim=2, activation='relu'))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "model.fit(X, y, epochs=0, batch_size=4)\n",
        "\n",
        "loss, accuracy = model.evaluate(X, y)\n",
        "print(\"Loss:\", loss)\n",
        "print(\"Accuracy:\", accuracy)\n",
        "\n",
        "predictions = model.predict(X)\n",
        "print(\"Predictions:\")\n",
        "print(predictions)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BYclVc7dCJ0Q",
        "outputId": "8d5117f3-7dc6-4667-ac77-9285d8b08743"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 386ms/step - loss: 0.6129 - accuracy: 0.7500\n",
            "Loss: 0.6128679513931274\n",
            "Accuracy: 0.75\n",
            "1/1 [==============================] - 0s 231ms/step\n",
            "Predictions:\n",
            "[[0.5       ]\n",
            " [0.45447773]\n",
            " [0.65146405]\n",
            " [0.41794205]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Implement House price prediction from kaggle dataset by using Multi Layered Perceptron."
      ],
      "metadata": {
        "id": "9rr8be4Qda4O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "\n",
        "data = pd.read_excel('file.xlsx')\n",
        "data=data.values\n",
        "X = data[:,0:10]\n",
        "y = data[:,10]\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "X_test_scaled\n",
        "model = Sequential([\n",
        "    Dense(128, activation='relu', input_shape=(X_train.shape[1],)),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mse'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=12, batch_size=32, validation_split=0.1)\n",
        "\n",
        "mse = model.evaluate(X_test, y_test)\n",
        "print(\"Mean Squared Error on Test Set:\", mse)\n",
        "\n",
        "predictions = model.predict(X_test)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gq1y9ISEddB8",
        "outputId": "165bfa50-d69c-48b7-ffdd-3b5fa973f4a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/12\n",
            "33/33 [==============================] - 2s 14ms/step - loss: 10747.6504 - mse: 10747.6504 - val_loss: 5314.5737 - val_mse: 5314.5737\n",
            "Epoch 2/12\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 28620.2559 - mse: 28620.2559 - val_loss: 4539.3184 - val_mse: 4539.3184\n",
            "Epoch 3/12\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 13702.4707 - mse: 13702.4707 - val_loss: 4224.6631 - val_mse: 4224.6631\n",
            "Epoch 4/12\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 39485.8789 - mse: 39485.8789 - val_loss: 56886.4375 - val_mse: 56886.4375\n",
            "Epoch 5/12\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 30424.1660 - mse: 30424.1660 - val_loss: 21291.6445 - val_mse: 21291.6445\n",
            "Epoch 6/12\n",
            "33/33 [==============================] - 0s 9ms/step - loss: 25950.7207 - mse: 25950.7207 - val_loss: 73741.2734 - val_mse: 73741.2734\n",
            "Epoch 7/12\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 29652.8223 - mse: 29652.8223 - val_loss: 1587.8022 - val_mse: 1587.8022\n",
            "Epoch 8/12\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 13402.2686 - mse: 13402.2686 - val_loss: 13825.8848 - val_mse: 13825.8848\n",
            "Epoch 9/12\n",
            "33/33 [==============================] - 0s 6ms/step - loss: 26790.1973 - mse: 26790.1973 - val_loss: 2033.0737 - val_mse: 2033.0737\n",
            "Epoch 10/12\n",
            "33/33 [==============================] - 0s 7ms/step - loss: 4921.1182 - mse: 4921.1182 - val_loss: 346.2377 - val_mse: 346.2377\n",
            "Epoch 11/12\n",
            "33/33 [==============================] - 0s 12ms/step - loss: 157.5779 - mse: 157.5779 - val_loss: 12.1114 - val_mse: 12.1114\n",
            "Epoch 12/12\n",
            "33/33 [==============================] - 0s 8ms/step - loss: 17.7110 - mse: 17.7110 - val_loss: 9.2679 - val_mse: 9.2679\n",
            "10/10 [==============================] - 0s 3ms/step - loss: 9.8953 - mse: 9.8953\n",
            "Mean Squared Error on Test Set: [9.895341873168945, 9.895341873168945]\n",
            "10/10 [==============================] - 0s 3ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.evaluate(X_test, y_test)[1]*10\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W7X0eZogWZkR",
        "outputId": "fc35d5e3-cba7-4092-f1d5-18e45c898916"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 3ms/step - loss: 9.8953 - mse: 9.8953\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "98.95341873168945"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VT8hbJj3Wa46"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "LSTM\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "PBHkBcv3pDLO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "sequence_length = 28  # MNIST image height\n",
        "input_size = 28  # MNIST image width\n",
        "hidden_size = 128\n",
        "num_layers = 2\n",
        "num_classes = 10\n",
        "batch_size = 100\n",
        "num_epochs = 1\n",
        "learning_rate = 0.001\n",
        "\n",
        "#Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# MNIST dataset\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                           train=True,\n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                          train=False,\n",
        "                                          transform=transforms.ToTensor())\n",
        "\n",
        "# Data loader\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
        "                                           batch_size=batch_size,\n",
        "                                           shuffle=True)\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
        "                                          batch_size=batch_size,\n",
        "                                          shuffle=False)\n",
        "\n",
        "# Define LSTM model\n",
        "class LSTMModel(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
        "        super(LSTMModel, self).__init__()\n",
        "        self.hidden_size = hidden_size\n",
        "        self.num_layers = num_layers\n",
        "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_size, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Set initial hidden and cell states\n",
        "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
        "\n",
        "        # Forward propagate LSTM\n",
        "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
        "\n",
        "        # Decode the hidden state of the last time step\n",
        "        out = self.fc(out[:, -1, :])\n",
        "        return out\n",
        "\n",
        "model = LSTMModel(input_size, hidden_size, num_layers, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
        "\n",
        "# Train the model\n",
        "total_step = len(train_loader)\n",
        "for epoch in range(num_epochs):\n",
        "    for i, (images, labels) in enumerate(train_loader):\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "# Test the model\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for images, labels in test_loader:\n",
        "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total))"
      ],
      "metadata": {
        "id": "iJn_99w2pEXH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f8a59fa-8373-461d-81c8-48f851c3669b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 41219941.93it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 55925989.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 33342612.76it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 15744238.65it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Test Accuracy of the model on the 10000 test images: 95.13 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "batch_size=100\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "train_dataset = torchvision.datasets.MNIST(root='./data',\n",
        "                                           train=True,\n",
        "                                           transform=transforms.ToTensor(),\n",
        "                                           download=True)\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(root =\"./data\",download=False,transform =transforms.ToTensor(),train=False)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,batch_size=batch_size,shuffle=True)"
      ],
      "metadata": {
        "id": "vRIvbLWCsb8d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qpc7YLhSwU-j"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}